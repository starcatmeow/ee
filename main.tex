\documentclass{article}

\title{Evaluating the Relevancy of JFIF Image Compression on the Internet}
\author{Uzen}
\date{Jul 22}

\usepackage{indentfirst}
\usepackage{tabularx,booktabs}
\usepackage[a4paper]{geometry}

\usepackage{tikz}
\usepackage{xcolor}

\usepackage{hyperref}
\hypersetup{
	linktoc    = all,
	colorlinks = true,
	urlcolor   = blue,
	linkcolor  = black,
	filecolor  = black,
	citecolor  = black,
	pdftitle   = {\@title},
	pdfauthor  = {\@author},
}

\usepackage[backend=biber,style=mla,showmedium=false]{biblatex}
\addbibresource{citations.bib}
\DeclareBibliographyDriver{footcite:online}{%
  \usedriver
    {\renewbibmacro*{bbx:savehash}{}}
    {online}}
\providecommand{\biburldatelong}{}

\usepackage{titlesec}
\newcommand{\sectionbreak}{\clearpage}

\parskip=1em

\begin{document}

\maketitle

{\LARGE \tt this is a DRAFT!!!!!!}

\tableofcontents
\newpage

\section{Introduction}
Compression is the cornerstone of the internet.
Behind the millions of bytes flowing between computers everyday around the world, compression algorithms are behind every single data packet.
Compression algorithms are tools that can seemingly magically shrink a digital document, sometimes until they are a tenth of their size.
Due the many uses of compression algorithms, they have been used in many different fields.
From shrinking large backups for storage to speeding up the transfer of data across the web, compression algorithms are crucial to modern computing.

% too personal here
One specific field that I was interested in was website hosting.
A personal website can be a good representation of themselves, similar to a more polished social media page.
For a period of time, I was quite interested in starting my own website, renting virtual servers from Aliyun to start my project.
However, I quickly ran into an issue.
In the modern times, technology has progressed at an exponential rate, storage and bandwidth speeds are increasing at an extraordinary rate.
It is common to see computers with hundreds of gigabytes of storage and gigabit connection to the internet.
% less personal
So it was quite a surprise for me when I found out that adding bandwidth and storage to my server costed money.
A small, affordable server would only come with 10 to 50 gigabytes of storage a measly 1 megabit bandwidth connection (or purchase bandwidth by the gigabit used).

Due to this limitation, the website I made was slow.
I couldn't upload many images or videos because the server would run out of storage, or make the website load at a snail's pace.
This made me very careful about the files I upload, especially the sizes of image and videos.
My curiosity eventually lead me to image compression.
When an image is stored, there's different file types:\texttt{~.png,~.jpg,~.gif}.
These are actually different methods of storing images, using different methods image compression techniques.
When creating my website, I had to choose what method of image compression to use.
Each different method has its advantages and disadvantages.
% add some conclusion from personal perspective

One format of compression I am very interested in is JFIF compression (commonly found in JPEG files).
JFIF uses a mixture of clever math and the characteristics of the human eye to shrink photos.
It is commonly used in digital cameras.
However, JFIF was originally invented over 30 years ago (August 1990\footcite{jpeg}), and have not account for the many modern technological advances.
For example, JFIF is bad at dealing with digital art and digital text due to limitation of the compression techniques.
In the recent years, newer and more efficient image compression algorithms have been created, such as Webp. 
% can cut some stuff here
In fact, JPEG has released a newer version of JFIF, called JPEG-2000.
Some has even used Artificial Intelligence to enhance compression rates\footcite{googleAI}.

Nevertheless, JFIF compression still remains as a dominant image compression technique.
This paper aims to compare JFIF with modern image compression formats to evaluate whether it is still a competitive algorithm.
The results of the paper would help website creators decide the best format to store images on their websites.

\section{Background Information}

\subsection{Lossless and Lossy Compression}
\noindent There are two major types of compression algorithms: lossy and lossless.

\subsubsection{Lossless Compression}
Lossless compression finds repetitive, similar patterns inside a file, condenses the data to the smallest possible size.
One example of lossless compression is LZ77\footcite{lzYT}.
Commonly used on plain text documents\footnote{\texttt{.txt} files}, LZ77 finds repeated words, and stores it for reference.
The next time the repeated word is used, LZ77 stores a pointer that points towards the last instance of word.
In this paragraph, the word \texttt{compression} is used 5 times.
LZ77 will only store the first \texttt{compression}, and save space from the rest.

Lossless compression is good for data with a lot of repetition.
It also preserves the data it compresses, which lossy compression cannot.
On the other hand, lossless compression is less efficient than lossy compression.

\subsubsection{Lossy Compression}
Lossy compression finds and remove redundant data.
Lossy compression is used often in audio and video data.
Due to its nature, lossy compression techniques varies greatly depending on the input data.
While some lossless compression algorithms are generic, lossy compression algorithms cannot be interchange across different data types.

Lossy compression is more efficient than lossless compression.
But it can degrade the quality of the data, as some data is thrown away.
It is not suitable for all types of data.
For example, we cannot use lossy compression on text, as every character matters in a sentence.

\subsection{Storing Images}
A pixel is a square block with a specific color.
When a grid of pixels are placed together, it forms an image.
An image is simply a 2D array of pixels.
And each pixel can be stored as color.
This method of storing images is called \texttt{bitmap}.
\texttt{Add cool diagram showing pixels and image.}

Color is generally represented in a color space\footnote{method of storing color as data}.
One commonly used color space is \texttt{RGB}\footnote{find rgb iso documents}, which stores the mixture of red, green, and blueness of a color.
Imagine that a color is composed by shining red, green, blue light on a black wall.
Where the number associated with each value is the brightness of the light.
The resulting image would be stored as a 2D array, each element containing a RGB value.
\texttt{Add cool image of RGB}.
\texttt{Add table to demonstrate.}

An alternative method of storing images is as mathematical equations.
Images created this way are called \texttt{vector images}.
Vector images have widely different use cases then bitmap images.
However, this image type is severely limited at depicting complicated images, and is only suitable for digital art.
Thus, this paper will work with bitmap images only.

\subsubsection{Need for Compression}
Storing images as arrays is fast and convenient, however it also results in large files.
For example, a $1920$ by $1080$ image would take up $5.8$MiB of data.
\texttt{add in appendix}.
Although this seems small, considering that most webpages have only kilobytes of data, images occupy a large amount of space. 
\texttt{Find website (reddit, amazon, apple, CSDN) to do example, use appendix to show calculations}.
Videos are stored as a series of pictures.
Using the same metrics as above, a 10 minute video would take up $104$GiB of space.
Uploading this to a websites would be extremely impractical.

\subsection{Image Formats}
Modern computer systems compresses images by default.
Different images files have been developed with different algorithms and uses in mind.
Below is a table of common image file types.

\begin{figure}
\footnotesize
\begin{tabularx}{\textwidth}{X X X X X X X}
\toprule
File Type & Extension & C. Method & C. Strength & Uses Case           & Availability & Cons \\
\midrule
\bf JFIF  & .jpg      & lossy     & medium      & still images        & high         & can cause compression artifacts              \\
\bf PNG   & .png      & lossless  & medium      & high quality images & high         & worse compression than lossless compression \\
\bf GIF   & .gif      & lossless  & low         & animated graphics   & high         & low performance                              \\
\bf AVIF  & .avif     & both      & high        & all                 & medium       & basic support on some web browsers           \\
\bf WebP  & .webp     & both      & high        & all                 & high         & slightly worse compression than AVIF         \\
\bf TIFF  & .tiff     & lossless  & low         & high quality images & medium       & support on some browsers only                \\
\bottomrule
\end{tabularx}
\texttt{split two tables, add full name}
\end{figure}

\subsection{Websites}
Since this paper focuses on the use of compression on the internet, this subsection will briefly introduce the concepts of websites.
Websites are hosted on rented cloud servers, with limitations on bandwidth, storage, processing power, and memory.
Images are rendered client side by web browser, thus processing power and memory is irrelevant.
Images require storage on the cloud server, large images may reduce the size of the website.
When serving a website, bandwidth may limit the speed of transfer.
Large images can increase loading times.
Web developers have to balance image quality with image size when creating a website.

Due to the many available web browsers, the standards for web development varies.
Web browsers support different image formats.
The most common web browser is Google Chrome\footnote{requires citation}, whilst many web browsers are based on Chrome.
When displaying an image, web browsers have to decompress, taking up processing power.
Although processing power is abundant on modern desktops, mobile devices and older computers may suffer.
Web developers should be aware of the requirements of different image formats.

\section{JPEG Methodology}

JFIF compression take advantages of two assumptions:\footcite{jpegCornell}
\begin{enumerate}
	\item Humans are more sensitive to brightness than color.
	\item Humans are not sensitive to high frequency contents.
\end{enumerate}

The first step in the algorithm is to transform the color space from \texttt{RGB} to \texttt{YCbCr}.
\texttt{Chroma Subsampling} (\ref{chroma}) is then performed on the image.
The image then undergoes \texttt{2D Discrete Cosine Transform} (\ref{dct}), and is \texttt{quantized} (\ref{quantize}).
Lastly, the resulting data matrix are compressed with \texttt{Huffman Encoding} (\ref{huffman}).

\subsection{Chroma Subsampling}\label{chroma}
Since the human eye is less sensitive to color than brightness, JFIF can downsample\footnote{reduce the size of an image} the color element of an image.
The commonly used \texttt{RGB} colorspace is convenient for projecting images, but does not separate the color from the luminance\footnote{brightness} of an image, \texttt{YCbCr} is used instead.
YCbCr separates color into \texttt{Y} (luminance, brightness of a pixel), \texttt{Cb} (chroma blue, blueness), and \texttt{Cr} (chroma red, redness).
Chroma green is not stored, as it can be calculated from Cb and Cr\footcite{ycbcrMedia}.
The resulted image will be a matrix of pixels with color data stored in YCbCR.

% Subsampling 4:4:4, 4:2:2, 4:2:0
% Add equation for rgb converting

\subsection{Discrete Cosine Transform}\label{dct}
Discrete Cosine Transform (DCT) separates an image into frequency components.
DCT is a subset of Fourier Transformation\footcite{dctYT}, which takes a time based function and transforms it into the frequency and amplitude components.
This is performed by first splitting an image into small 8 by 8 pixel chunks, then performing the algorithm.
Although the DCT used on imaged are 2 dimensions, we can explain the concept from a 1 dimension perspective first.

Lets take a single row of 8 pixels, and plot them on a graph with pixels on the x axis and intensity on the y axis.
The resulted data points forms a signal.
If we were to recreate the curve mathematically, we can plot a curve through the data points.
DCT reconstructs the signal by stacking differently weighted cosine curves of different frequency and amplitude.

For example, figure~\ref{fig:cos1} shows a cosine curve with a period of 8 pixels.
Lets say this curve has a frequency of 1 unit.
The purple coordinates represents the resultant signal.
\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{./figures/dct/cos1}
	\caption{Cosine curve with frequency of 1 unit}
	\label{fig:cos1}
\end{figure}

If we add an additional cosine curve with a frequency of 2 units, the signal will be changed.
In this case, the signal is the average of both curves.
\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{./figures/dct/cos2}
	\caption{Cosine curves with frequency 1 and 2 unit}
	\label{fig:cos2}
\end{figure}

If we change to weighting of the average, we can change the signal.
In this case, the cosine curve with a frequency of 1 unit is weighed 3 times as much as the other cosine curve.
\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{./figures/dct/cos3}
	\caption{Weighted cosine curves}
	\label{fig:cos3}
\end{figure}

DCT\footnote{More specifically DCT-II, used by JIFF compression} propose that we can represent any signal by adding 8 weighted cosine curves of increasing frequency.
The resulted signal is modeled by this mathematical equation.
The exact mathematics of DCT is too advance for the scope of this paper, implementation of DCT will be done through code.
\begin{equation}
	X_k = \sum_{n=0}^{N-1} x_n \cos \left [ \frac{\pi}{N} \left ( n + \frac{1}{2} \right ) \right ] \qquad k = 0, \dots , N - 1
\end{equation}

% explain why with notation table

\texttt{Continue with an example here (use random number and find the frequency)}

The example showed 1 dimension DCT, 2 dimention DCT is used on photos.

\subsection{Quantization}\label{quantize}

\subsection{Huffman Encoding}\label{huffman}

\printbibliography

\end{document}
